# @calltelemetry/openclaw-linear

[![OpenClaw](https://img.shields.io/badge/OpenClaw-v2026.2+-blue)](https://github.com/calltelemetry/openclaw)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

Connect Linear to AI agents. Issues get triaged, implemented, and audited â€” automatically.

---

## What It Does

- **New issue?** Agent estimates story points, adds labels, sets priority.
- **Assign to agent?** A worker implements it, an independent auditor verifies it, done.
- **Comment anything?** The bot understands natural language â€” no magic commands needed.
- **Say "close this" or "mark as done"?** Agent writes a closure report and transitions the issue to completed.
- **Say "let's plan the features"?** A planner interviews you, writes user stories, and builds your full issue hierarchy.
- **Plan looks good?** A different AI model automatically audits the plan before dispatch.
- **Agent goes silent?** A watchdog kills it and retries automatically.
- **Linear guidance?** Workspace and team-level guidance from Linear flows into every agent prompt â€” triage, dispatch, worker, audit.
- **Want updates?** Get notified on Discord, Slack, Telegram, or Signal.

---

## Quick Start

### 1. Install the plugin

```bash
openclaw plugins install @calltelemetry/openclaw-linear
```

### 2. Create a Linear OAuth app

Go to **Linear Settings > API > Applications** and create an app:

- Set **Webhook URL** to `https://your-domain.com/linear/webhook`
- Set **Redirect URI** to `https://your-domain.com/linear/oauth/callback`
- Enable events: **Agent Sessions**, **Comments**, **Issues**
- Save your **Client ID** and **Client Secret**

> You also need a **workspace webhook** â€” run `openclaw openclaw-linear webhooks setup` to auto-provision it, or manually create one in Settings > API > Webhooks pointing to the same URL with **Comment + Issue** events enabled. Both webhooks are required.

### 3. Set credentials

```bash
export LINEAR_CLIENT_ID="your_client_id"
export LINEAR_CLIENT_SECRET="your_client_secret"
```

For systemd services, add these to your unit file:

```ini
[Service]
Environment=LINEAR_CLIENT_ID=your_client_id
Environment=LINEAR_CLIENT_SECRET=your_client_secret
```

Then reload: `systemctl --user daemon-reload && systemctl --user restart openclaw-gateway`

### 4. Authorize

```bash
openclaw openclaw-linear auth
```

This opens your browser. Approve the authorization, then restart:

```bash
systemctl --user restart openclaw-gateway
```

### 5. Verify

```bash
openclaw openclaw-linear status
```

You should see a valid token and connected status. Check the gateway logs for a clean startup:

```
Linear agent extension registered (agent: default, token: profile, orchestration: enabled)
```

Test the webhook endpoint:

```bash
curl -s -X POST https://your-domain.com/linear/webhook \
  -H "Content-Type: application/json" \
  -d '{"type":"test","action":"ping"}'
# Returns: "ok"
```

That's it. Create an issue in Linear and watch the agent respond.

---

## How It Works â€” Step by Step

Every issue moves through a clear pipeline. Here's exactly what happens at each stage and what you'll see in Linear.

```
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Triage  â”‚â”€â”€â”€â–¶â”‚ Dispatch â”‚â”€â”€â”€â–¶â”‚ Worker â”‚â”€â”€â”€â–¶â”‚ Audit â”‚â”€â”€â”€â–¶â”‚  Done âœ”  â”‚
 â”‚(auto)   â”‚    â”‚(you      â”‚    â”‚(auto)  â”‚    â”‚(auto) â”‚    â”‚          â”‚
 â”‚         â”‚    â”‚ assign)  â”‚    â”‚        â”‚    â”‚       â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”¬â”€â”€â”€â”˜
                                                  â”‚
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                                    â–¼             â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚ Rework   â”‚  â”‚ Needs Your    â”‚
                              â”‚ (auto    â”‚  â”‚ Help âš         â”‚
                              â”‚  retry)  â”‚  â”‚ (escalated)   â”‚
                              â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â””â”€â”€â–¶ back to Worker
```

### Stage 1: Triage (automatic)

**Trigger:** You create a new issue.

The agent reads your issue, estimates story points, adds labels, sets priority, and posts an assessment comment â€” all within seconds. Triage runs in **read-only mode** (no file writes, no code execution) to prevent side effects.

**What you'll see in Linear:**

> **[Mal]** This looks like a medium complexity change â€” the search API integration touches both the backend GraphQL schema and the frontend query layer. I've estimated 3 points and tagged it `backend` + `frontend`.

The estimate, labels, and priority are applied silently to the issue fields. You don't need to do anything.

### Stage 2: Dispatch (you assign the issue)

**Trigger:** You assign the issue to the agent (or it gets auto-assigned after planning).

The agent assesses complexity, picks an appropriate model, creates an isolated git worktree, and starts working.

**What you'll see in Linear:**

> **Dispatched** as **senior** (anthropic/claude-opus-4-6)
> > Complex multi-service refactor with migration concerns
>
> Worktree: `/home/claw/worktrees/ENG-100` (fresh)
> Branch: `codex/ENG-100`
>
> **Status:** Worker is starting now. An independent audit runs automatically after implementation.
>
> **While you wait:**
> - Check progress: `/dispatch status ENG-100`
> - Cancel: `/dispatch escalate ENG-100 "reason"`
> - All dispatches: `/dispatch list`

**Complexity tiers:**

| Tier | Model | When |
|---|---|---|
| Junior | claude-haiku-4-5 | Simple config changes, typos, one-file fixes |
| Medior | claude-sonnet-4-6 | Standard features, multi-file changes |
| Senior | claude-opus-4-6 | Complex refactors, architecture changes |

### Stage 3: Implementation (automatic)

The worker agent reads the issue, plans its approach, writes code, and runs tests â€” all in the isolated worktree. You don't need to do anything during this stage.

If this is a **retry** after a failed audit, the worker gets the previous audit feedback as context so it knows exactly what to fix.

**Notifications you'll receive:**
> `ENG-100 working on it (attempt 1)`

### Stage 4: Audit (automatic)

After the worker finishes, a separate auditor agent independently verifies the work. The auditor checks the issue requirements against what was actually implemented.

This is **not optional** â€” the worker cannot mark its own work as done. The audit is triggered by the plugin, not by the AI.

**Notifications you'll receive:**
> `ENG-100 checking the work...`

### Stage 5: Verdict

The audit produces one of three outcomes:

#### Pass â€” Issue is done

The issue is marked done automatically. A summary is posted.

**What you'll see in Linear:**

> ## Done
>
> This issue has been implemented and verified.
>
> **What was checked:**
> - API endpoint returns correct response format
> - Tests pass for edge cases
> - Error handling covers timeout scenarios
>
> **Test results:** 14 tests passed, 0 failed
>
> ---
> *Completed on attempt 1. Artifacts: `/home/claw/worktrees/ENG-100/.claw/`*

**Notification:** `âœ… ENG-100 done! Ready for review.`

#### Fail (retries left) â€” Automatic rework

The worker gets the audit feedback and tries again. You don't need to do anything.

**What you'll see in Linear:**

> ## Needs More Work
>
> The implementation was checked and some things need to be addressed. Retrying automatically.
>
> **Attempt 1 of 3**
>
> **What needs fixing:**
> - Missing input validation on the search endpoint
> - No test for empty query string
>
> **Test results:** 12 passed, 2 failed

**Notification:** `ENG-100 needs more work (attempt 1). Issues: missing validation, no empty query test`

#### Fail (no retries left) â€” Needs your help

After all retries are exhausted (default: 3 attempts), the issue is escalated to you.

**What you'll see in Linear:**

> ## Needs Your Help
>
> The automatic retries didn't fix these issues:
>
> **What went wrong:**
> - Search pagination still returns duplicate results
> - Integration test flaky on CI
>
> **Test results:** 10 passed, 4 failed
>
> ---
> *Please review and either:*
> - *Update the issue description with clearer requirements, then re-assign*
> - *Fix the issues manually in the worktree at `/home/claw/worktrees/ENG-100`*

**Notification:** `ğŸš¨ ENG-100 needs your help â€” couldn't fix it after 3 tries`

**What you can do:**
1. **Clarify the issue** â€” Add more detail to the description, then re-assign to try again
2. **Fix it yourself** â€” The agent's work is in the worktree, ready to edit
3. **Force retry** â€” `/dispatch retry ENG-100`
4. **Check logs** â€” Worker output in `.claw/worker-*.md`, audit verdicts in `.claw/audit-*.json`

### Stage 6: Timeout (if the agent goes silent)

If the agent produces no output for 2 minutes (configurable), the watchdog kills it and retries once. If the retry also times out, the issue is escalated.

**What you'll see in Linear:**

> ## Agent Timed Out
>
> The agent stopped responding for over 120s and was automatically restarted, but the retry also failed.
>
> **What to do:** Re-assign this issue to try again. If it keeps timing out, the issue might be too complex â€” try breaking it into smaller issues.

**Notification:** `âš¡ ENG-100 timed out (no activity for 120s). Will retry.`

### What's in the worktree

Every dispatch creates a `.claw/` folder inside the worktree with everything the agent did:

```
/home/claw/worktrees/ENG-100/
â”œâ”€â”€ .claw/
â”‚   â”œâ”€â”€ manifest.json       # Issue metadata, tier, status, attempt count
â”‚   â”œâ”€â”€ worker-0.md         # What the worker did on attempt 1
â”‚   â”œâ”€â”€ worker-1.md         # What the worker did on attempt 2 (if retried)
â”‚   â”œâ”€â”€ audit-0.json        # Audit verdict for attempt 1
â”‚   â”œâ”€â”€ audit-1.json        # Audit verdict for attempt 2
â”‚   â”œâ”€â”€ log.jsonl           # Timeline of every phase with timing
â”‚   â””â”€â”€ summary.md          # Final summary (written on done or stuck)
â”œâ”€â”€ src/                    # â† your code, modified by the agent
â”œâ”€â”€ tests/
â””â”€â”€ ...
```

If something went wrong, start with `log.jsonl` â€” it shows every phase, how long it took, and a preview of the output.

---

## Comment Routing â€” Talk Naturally

You don't need to memorize magic commands. The bot uses an LLM-based intent classifier to understand what you want from any comment.

```
User comment â†’ Intent Classifier (small model, ~2s) â†’ Route to handler
                         â†“ (on failure)
                    Regex fallback â†’ Route to handler
```

**What the bot understands:**

| What you say | What happens |
|---|---|
| "let's plan the features for this" | Starts planning interview |
| "looks good, ship it" (during planning) | Runs plan audit + cross-model review |
| "nevermind, cancel this" (during planning) | Exits planning mode |
| "hey kaylee can you look at this?" | Routes to Kaylee (no `@` needed) |
| "@mal close this issue" | Routes to Mal (one-time detour) and closes the issue |
| "what can I do here?" | Default agent responds (not silently dropped) |
| "fix the search bug" | Default agent dispatches work |
| "close this" / "mark as done" / "this is resolved" | Generates closure report, transitions issue to completed |

`@mentions` still work as a fast path â€” if you write `@kaylee`, the classifier is skipped entirely for speed.

> **Tip:** Configure `classifierAgentId` to point to a small/fast model agent (like Haiku) for low-latency, low-cost intent classification. The classifier only needs ~300 tokens per call.

### Agent Routing

The plugin supports a multi-agent team where one agent is the default (`isDefault: true` in agent profiles) and others are routed to on demand. Routing works across all webhook paths:

| Webhook Path | How agent is selected |
|---|---|
| `Comment.create` | `@mention` in comment text â†’ specific agent. No mention â†’ intent classifier may detect agent name ("hey kaylee") â†’ `ask_agent` intent. Otherwise â†’ default agent. |
| `AgentSessionEvent.created` | Scans user's message for `@mention` aliases â†’ routes to mentioned agent for that interaction. No mention â†’ default agent. |
| `AgentSessionEvent.prompted` | Same as `created` â€” scans follow-up message for `@mention` â†’ one-time detour to mentioned agent. No mention â†’ default agent. |
| `Issue.update` (assignment) | Always dispatches to default agent. |
| `Issue.create` (triage) | Always dispatches to default agent. |

**One-time detour:** When you `@mention` an agent in a session that belongs to a different default agent, the mentioned agent handles that single interaction. The session itself stays owned by whoever created it â€” subsequent messages without `@mentions` go back to the default. This lets you ask a specific agent for help without permanently switching context.

**Agent profiles** are configured in `~/.openclaw/agent-profiles.json`:

```json
{
  "agents": {
    "mal": {
      "label": "Mal",
      "mentionAliases": ["mal"],
      "isDefault": false
    },
    "zoe": {
      "label": "Zoe",
      "mentionAliases": ["zoe"],
      "isDefault": true
    }
  }
}
```

Each agent needs a unique set of `mentionAliases`. The `appAliases` field (e.g. `["ctclaw"]`) is separate â€” those trigger `AgentSessionEvent` from Linear's own `@app` mention system, not the plugin's routing.

### Deduplication

The webhook handler prevents double-processing through a two-tier guard system:

1. **`activeRuns` (in-memory Set)** â€” O(1) check if an agent is already running for an issue. Catches feedback loops where our own API calls (e.g., `createComment`, `createSessionOnIssue`) trigger webhooks back to us.

2. **`wasRecentlyProcessed` (TTL Map, 60s)** â€” Catches exact-duplicate webhook deliveries. Each event type uses a specific dedup key:

| Event | Dedup Key | Guards (in order) |
|---|---|---|
| `AgentSessionEvent.created` | `session:<sessionId>` | activeRuns â†’ wasRecentlyProcessed |
| `AgentSessionEvent.prompted` | `webhook:<webhookId>` | activeRuns â†’ wasRecentlyProcessed |
| `Comment.create` | `comment:<commentId>` | wasRecentlyProcessed â†’ viewerId â†’ activeRuns |
| `Issue.update` | `<trigger>:<issueId>:<viewerId>` | activeRuns â†’ no-change â†’ viewerId â†’ wasRecentlyProcessed |
| `Issue.create` | `issue-create:<issueId>` | wasRecentlyProcessed â†’ activeRuns â†’ planning mode â†’ bot-created |
| `AppUserNotification` | *(immediate discard)* | â€” |

`AppUserNotification` events are discarded because they duplicate events already received via the workspace webhook (e.g., `Comment.create` for mentions, `Issue.update` for assignments). Processing both would cause double agent runs.

**Response delivery:** When an agent session exists, responses are delivered via `emitActivity(type: "response")` â€” not `createComment`. This prevents duplicate visible messages on the issue. `createComment` is only used as a fallback when `emitActivity` fails or when no agent session exists.

**Comment echo prevention:** Comments posted outside of sessions use `createCommentWithDedup()`, which pre-registers the comment's ID in `wasRecentlyProcessed` immediately after the API returns. When Linear echoes the `Comment.create` webhook back, it's caught before any processing.

---

## Planning a Project

For larger work, the planner helps you break a project into issues with dependencies, then dispatches them automatically.

### Start planning

Comment on any issue that belongs to a Linear project â€” use natural language:

> "let's plan out the features for this project"

The planner enters **interview mode** and asks you questions one at a time:

> I'm entering planning mode for **Search Feature**. I'll interview you about the features you want to build, then structure everything into Linear issues.
>
> Let's start â€” what is this project about, and what are the main feature areas?

### Build the plan

Reply with your ideas. The planner creates issues with **user stories** and **acceptance criteria**, sets dependencies, and asks follow-up questions:

> I've created 3 issues:
> - **PROJ-2:** Build search API endpoint (3 pts, blocks PROJ-3)
> - **PROJ-3:** Search results page (2 pts, blocked by PROJ-2)
> - **PROJ-4:** Autocomplete suggestions (1 pt, independent)
>
> For PROJ-2, here's what I wrote for acceptance criteria:
> - *Given* a user sends a search query, *When* results exist, *Then* they are returned with pagination
>
> Does that cover it? Should the autocomplete call a separate endpoint or share the search API?

The planner proactively asks for:
- **User stories** â€” "As a [role], I want [feature] so that [benefit]"
- **Acceptance criteria** â€” Given/When/Then format
- **UAT test scenarios** â€” How to manually verify the feature

Keep replying until the plan looks right. The planner updates issues in real time.

### Finalize & Cross-Model Review

When you're happy, say something like "looks good" or "finalize plan". The planner runs a validation check:
- Every issue has a description (50+ characters) with acceptance criteria
- Every non-epic issue has an estimate and priority
- No circular dependencies in the DAG

**If validation passes, a cross-model review runs automatically:**

> ## Plan Passed Checks
>
> **3 issues** with valid dependency graph.
>
> Let me have **Codex** audit this and make recommendations.

A different AI model (always the complement of your primary model) reviews the plan for gaps:

| Your primary model | Auto-reviewer |
|---|---|
| Claude / Anthropic | Codex |
| Codex / OpenAI | Gemini |
| Gemini / Google | Codex |
| Other (Kimi, Mistral, etc.) | Gemini |

After the review, the planner summarizes recommendations and asks you to approve:

> Codex suggested adding error handling scenarios to PROJ-2 and noted PROJ-4 could be split into frontend/backend. I've updated PROJ-2's acceptance criteria. The PROJ-4 split is optional â€” your call.
>
> If you're happy with this plan, say **approve plan** to start dispatching.

**If validation fails:**

> ## Plan Audit Failed
>
> **Problems:**
> - PROJ-2: description too short (< 50 chars)
> - PROJ-3: missing estimate
>
> **Warnings:**
> - PROJ-4: no acceptance criteria found in description
>
> Please address these issues, then say "finalize plan" again.

Fix the issues and try again. You can also say "cancel" or "stop planning" to exit without dispatching.

### DAG dispatch progress

After approval, issues are assigned to the agent automatically in dependency order. Up to 3 issues run in parallel.

> `ğŸ“Š Search Feature: 2/3 complete`

When everything is done:

> `âœ… Search Feature: complete (3/3 issues)`

If an issue gets stuck (all retries failed), dependent issues are blocked and you'll be notified.

---

## Quick Reference

| What you do in Linear | What happens |
|---|---|
| Create a new issue | Agent triages â€” adds estimate, labels, priority |
| Assign an issue to the agent | Worker â†’ Audit â†’ Done (or retry, or escalate) |
| Comment anything on an issue | Intent classifier routes to the right handler |
| Mention an agent by name (with or without `@`) | That agent responds |
| Ask a question or request work | Default agent handles it |
| Say "close this" / "mark as done" / "this is resolved" | Closure report posted, issue moved to completed |
| Say "plan this project" (on a project issue) | Planning interview starts |
| Reply during planning | Issues created/updated with user stories & AC |
| Say "looks good" / "finalize plan" | Validates â†’ cross-model review â†’ approval |
| Say "approve plan" (after review) | Dispatches all issues in dependency order |
| Say "cancel" / "abandon planning" | Exits planning mode |
| `/dispatch list` | Shows all active dispatches |
| `/dispatch retry CT-123` | Re-runs a stuck dispatch |
| `/dispatch status CT-123` | Detailed dispatch info |
| Add `<!-- repos: api, frontend -->` to issue body | Multi-repo dispatch |

---

## Configuration

Add settings under the plugin entry in `openclaw.json`:

```json
{
  "plugins": {
    "entries": {
      "openclaw-linear": {
        "config": {
          "defaultAgentId": "coder",
          "maxReworkAttempts": 2,
          "enableAudit": true
        }
      }
    }
  }
}
```

### Plugin Settings

| Key | Type | Default | What it does |
|---|---|---|---|
| `defaultAgentId` | string | `"default"` | Which agent runs the pipeline |
| `classifierAgentId` | string | â€” | Agent for intent classification (use a small/fast model like Haiku) |
| `plannerReviewModel` | string | auto | Cross-model plan reviewer: `"claude"`, `"codex"`, or `"gemini"`. Auto-detects the complement of your primary model. |
| `enableAudit` | boolean | `true` | Run auditor after implementation |
| `enableOrchestration` | boolean | `true` | Allow `spawn_agent` / `ask_agent` tools |
| `maxReworkAttempts` | number | `2` | Max audit failures before escalation |
| `codexBaseRepo` | string | `"/home/claw/ai-workspace"` | Git repo for worktrees |
| `worktreeBaseDir` | string | `"~/.openclaw/worktrees"` | Where worktrees are created |
| `repos` | object | â€” | Multi-repo map (see [Multi-Repo](#multi-repo)) |
| `dispatchStatePath` | string | `"~/.openclaw/linear-dispatch-state.json"` | Dispatch state file |
| `planningStatePath` | string | `"~/.openclaw/linear-planning-state.json"` | Planning session state file |
| `promptsPath` | string | â€” | Custom prompts file path |
| `notifications` | object | â€” | Notification targets (see [Notifications](#notifications)) |
| `inactivitySec` | number | `120` | Kill agent if silent this long |
| `maxTotalSec` | number | `7200` | Max total agent session time |
| `toolTimeoutSec` | number | `600` | Max single `code_run` time |
| `enableGuidance` | boolean | `true` | Inject Linear workspace/team guidance into agent prompts |
| `teamGuidanceOverrides` | object | â€” | Per-team guidance toggle. Key = team ID, value = boolean. Unset teams inherit `enableGuidance`. |
| `claudeApiKey` | string | â€” | Anthropic API key for Claude CLI (passed as `ANTHROPIC_API_KEY` env var). Required if using Claude backend. |

### Environment Variables

| Variable | Required | What it does |
|---|---|---|
| `LINEAR_CLIENT_ID` | Yes | OAuth app client ID |
| `LINEAR_CLIENT_SECRET` | Yes | OAuth app client secret |
| `LINEAR_API_KEY` | No | Personal API key (fallback) |

### Agent Profiles

Define your agents in `~/.openclaw/agent-profiles.json`:

```json
{
  "agents": {
    "coder": {
      "label": "Coder",
      "mission": "Full-stack engineer. Plans, implements, ships.",
      "isDefault": true,
      "mentionAliases": ["coder"],
      "avatarUrl": "https://example.com/coder.png",
      "watchdog": {
        "inactivitySec": 180,
        "maxTotalSec": 7200,
        "toolTimeoutSec": 900
      }
    },
    "qa": {
      "label": "QA",
      "mission": "Test engineer. Reviews code, writes tests.",
      "mentionAliases": ["qa", "tester"]
    }
  }
}
```

One agent must have `"isDefault": true` â€” that's the one that handles triage and the dispatch pipeline.

### Coding Tools

Create `coding-tools.json` in the plugin root to configure which CLI backend agents use:

> **Warning â€” Claude Code (Anthropic) and headless/automated usage**
>
> Calling Claude Code via CLI in a headless or automated context (which is how this plugin
> uses it) may violate [Anthropic's Terms of Service](https://www.anthropic.com/terms).
> The default backend is **Codex CLI** (OpenAI). **Gemini CLI** (Google) is used as the
> cross-model reviewer. If you choose to use Claude despite this, you do so at your own risk.
> See [Claude API Key](#claude-api-key) below for opt-in configuration.

```json
{
  "codingTool": "codex",
  "agentCodingTools": {},
  "backends": {
    "claude": { "aliases": ["claude", "claude code", "anthropic"] },
    "codex": { "aliases": ["codex", "openai"] },
    "gemini": { "aliases": ["gemini", "google"] }
  }
}
```

The agent calls `code_run` without knowing which backend is active. Resolution order: explicit `backend` parameter > per-agent override > global default > `"codex"`.

#### Claude API Key

If you opt in to using Claude as a backend (despite the TOS concerns noted above), you can
provide an Anthropic API key so the Claude CLI authenticates via API key instead of its
built-in interactive auth.

Set `claudeApiKey` in the plugin config:

```json
{
  "plugins": {
    "entries": {
      "openclaw-linear": {
        "config": {
          "claudeApiKey": "sk-ant-..."
        }
      }
    }
  }
}
```

The key is passed to the Claude CLI subprocess as the `ANTHROPIC_API_KEY` environment variable.
You can also set `ANTHROPIC_API_KEY` as a process-level environment variable (e.g., in your
systemd unit file) as a fallback. The plugin config value takes precedence if both are set.

---

## Notifications

Get notified when dispatches start, pass audit, fail, or get stuck.

### Setup

```json
{
  "notifications": {
    "targets": [
      { "channel": "discord", "target": "1471743433566715974" },
      { "channel": "telegram", "target": "-1003884997363" },
      { "channel": "slack", "target": "C0123456789", "accountId": "my-acct" }
    ],
    "events": {
      "auditing": false
    },
    "richFormat": true
  }
}
```

- **`targets`** â€” Where to send notifications (channel name + ID)
- **`events`** â€” Toggle specific events off (all on by default)
- **`richFormat`** â€” Set to `true` for Discord embeds with colors and Telegram HTML formatting

### Events

| Event | When it fires |
|---|---|
| `dispatch` | Issue dispatched to pipeline |
| `working` | Worker started |
| `auditing` | Audit in progress |
| `audit_pass` | Audit passed, issue done |
| `audit_fail` | Audit failed, worker retrying |
| `escalation` | Too many failures, needs human |
| `stuck` | Dispatch stale for 2+ hours |
| `watchdog_kill` | Agent killed for inactivity |

### Test It

```bash
openclaw openclaw-linear notify test              # Test all targets
openclaw openclaw-linear notify test --channel discord  # Test one channel
openclaw openclaw-linear notify status             # Show config
```

---

## Prompt Customization

Worker, audit, and rework prompts live in `prompts.yaml`. You can customize them without rebuilding.

### Three Layers

Prompts merge in this order (later layers override earlier ones):

1. **Built-in defaults** â€” Ship with the plugin, always available
2. **Your global file** â€” Set `promptsPath` in config to point to your custom YAML
3. **Per-project file** â€” Drop a `prompts.yaml` in the worktree's `.claw/` folder

Each layer only overrides the specific sections you define. Everything else keeps its default.

### Linear Guidance

Linear's [agent guidance system](https://linear.app/docs/agents-in-linear) lets admins configure workspace-wide and team-specific instructions for agents. This plugin automatically extracts that guidance and appends it as supplementary instructions to all agent prompts.

Guidance is configured in Linear at:
- **Workspace level:** Settings > Agents > Additional guidance (applies across entire org)
- **Team level:** Team settings > Agents > Additional guidance (takes priority over workspace guidance)

See [Agents in Linear](https://linear.app/docs/agents-in-linear) for full documentation on how guidance works.

Guidance flows into:
- **Orchestrator prompts** â€” AgentSessionEvent and comment handler paths
- **Worker prompts** â€” Appended to the task via `{{guidance}}` template variable
- **Audit prompts** â€” Appended to the audit task
- **Triage and closure prompts** â€” Appended to the triage and close_issue handlers

Guidance is cached per-team (24h TTL) so comment webhooks (which don't carry guidance from Linear) can also benefit.

**Disable guidance globally:**
```json
{ "enableGuidance": false }
```

**Disable for a specific team:**
```json
{
  "enableGuidance": true,
  "teamGuidanceOverrides": { "team-id-here": false }
}
```

### Example Custom Prompts

```yaml
worker:
  system: "You are a senior engineer. Write clean, tested code."
  task: |
    Issue: {{identifier}} â€” {{title}}

    {{description}}

    Workspace: {{worktreePath}}

    Implement this issue. Write tests. Commit your work.

audit:
  system: "You are a strict code auditor."

rework:
  addendum: |
    PREVIOUS AUDIT FAILED. Fix these gaps:
    {{gaps}}
```

### Template Variables

| Variable | What it contains |
|---|---|
| `{{identifier}}` | Issue ID (e.g., `API-123`) |
| `{{title}}` | Issue title |
| `{{description}}` | Full issue body |
| `{{worktreePath}}` | Path to the git worktree |
| `{{tier}}` | Complexity tier (junior/medior/senior) |
| `{{attempt}}` | Current attempt number |
| `{{gaps}}` | Audit gaps from previous attempt |
| `{{projectName}}` | Project name (planner prompts) |
| `{{planSnapshot}}` | Current plan structure (planner prompts) |
| `{{reviewModel}}` | Name of cross-model reviewer (planner review) |
| `{{crossModelFeedback}}` | Review recommendations (planner review) |
| `{{guidance}}` | Linear workspace/team guidance (if available, empty string otherwise) |

### CLI

```bash
openclaw openclaw-linear prompts show       # View current prompts
openclaw openclaw-linear prompts path       # Show file path
openclaw openclaw-linear prompts validate   # Check for errors
```

---

## Multi-Repo

Sometimes a feature touches more than one repo â€” your API and your frontend, for example. Multi-repo lets the agent work on both at the same time, in separate worktrees.

### Step 1: Tell the plugin where your repos live

Add a `repos` map to your plugin config in `openclaw.json`. The **key** is a short name you pick, the **value** is the absolute path to that repo on disk:

```json
{
  "plugins": {
    "entries": {
      "openclaw-linear": {
        "config": {
          "repos": {
            "api": "/home/claw/repos/api",
            "frontend": "/home/claw/repos/frontend",
            "shared": "/home/claw/repos/shared-libs"
          }
        }
      }
    }
  }
}
```

Restart the gateway after saving: `systemctl --user restart openclaw-gateway`

### Step 1.5: Sync labels to Linear (optional)

If you plan to use labels (Method B below) to tag issues, run this to create the `repo:xxx` labels automatically:

```bash
openclaw openclaw-linear repos sync
```

This reads your `repos` config and creates matching labels (`repo:api`, `repo:frontend`, etc.) in every Linear team. To preview without creating anything:

```bash
openclaw openclaw-linear repos check
```

The check command also validates your repo paths â€” it'll warn you if a path doesn't exist, isn't a git repo, or is a **submodule** (which won't work with multi-repo dispatch).

### Step 2: Tag the issue

When you write an issue in Linear that needs multiple repos, tell the plugin which ones. Pick **one** of these methods:

#### Method A: HTML comment in the issue body (recommended)

Put this line anywhere in the issue description â€” it's invisible in Linear's UI:

```
<!-- repos: api, frontend -->
```

Full example of what an issue body might look like:

```
The search endpoint needs to be added to the API, and the frontend
needs a new search page that calls it.

<!-- repos: api, frontend -->

Acceptance criteria:
- GET /api/search?q=term returns results
- /search page renders results with pagination
```

#### Method B: Linear labels

Create labels in Linear called `repo:api` and `repo:frontend`, then add them to the issue. The part after `repo:` must match the key in your config.

#### Method C: Do nothing (config default)

If you don't tag the issue at all, the plugin uses your `codexBaseRepo` setting (a single repo). This is how it worked before multi-repo existed â€” nothing changes for single-repo issues.

### What happens when you dispatch

When the agent picks up a multi-repo issue, the dispatch comment tells you:

> **Dispatched** as **senior** (anthropic/claude-opus-4-6)
>
> Worktrees:
> - `api` â†’ `/home/claw/worktrees/ENG-100/api`
> - `frontend` â†’ `/home/claw/worktrees/ENG-100/frontend`
>
> Branch: `codex/ENG-100`

The agent gets access to all the worktrees and can edit files across repos in one session. Each repo gets its own git branch.

### Priority order

If an issue has both a body marker and labels, the body marker wins. Full order:

1. `<!-- repos: ... -->` in the issue body
2. `repo:xxx` labels on the issue
3. `codexBaseRepo` from config (single repo fallback)

### Common mistakes

| Problem | Fix |
|---|---|
| Agent only sees one repo | The name in `<!-- repos: api -->` must exactly match a key in your `repos` config. Check spelling. |
| "Could not create worktree" error | The path in your `repos` config doesn't exist, or it's not a git repo. Run `ls /home/claw/repos/api/.git` to check. |
| Comment marker not detected | Must be `<!-- repos: name1, name2 -->` with the exact format. No extra spaces around `<!--` or `-->`. |
| Labels not picked up | Labels must be formatted `repo:name` (lowercase, no spaces). The `name` part must match a `repos` config key. |

---

## Dispatch Management

### Slash Commands

Type these in any agent session â€” they run instantly, no AI involved:

| Command | What it does |
|---|---|
| `/dispatch list` | Show all active dispatches with age, tier, status |
| `/dispatch status CT-123` | Detailed info for one dispatch |
| `/dispatch retry CT-123` | Re-run a stuck dispatch |
| `/dispatch escalate CT-123 "needs review"` | Force a dispatch to stuck status |

### Gateway API

For programmatic access, the plugin registers these RPC methods:

| Method | What it does |
|---|---|
| `dispatch.list` | List dispatches (filterable by status, tier) |
| `dispatch.get` | Get full dispatch details |
| `dispatch.retry` | Re-dispatch a stuck issue |
| `dispatch.escalate` | Force-stuck with a reason |
| `dispatch.cancel` | Remove an active dispatch |
| `dispatch.stats` | Counts by status and tier |

---

## Watchdog

If an agent goes silent (LLM timeout, API hang, CLI lockup), the watchdog handles it automatically:

1. No output for `inactivitySec` â†’ kill and retry once
2. Second silence â†’ escalate to stuck (you get notified, see [Stage 6](#stage-6-timeout-if-the-agent-goes-silent) above)

| Setting | Default | What it controls |
|---|---|---|
| `inactivitySec` | 120s | Kill if no output for this long |
| `maxTotalSec` | 7200s (2 hrs) | Hard ceiling on total session time |
| `toolTimeoutSec` | 600s (10 min) | Max time for a single `code_run` call |

Configure per-agent in `agent-profiles.json` or globally in plugin config.

---

## Agent Tools

Every agent session gets these registered tools. They're available as native tool calls â€” no CLI parsing, no shell execution, no flag guessing.

### `code_run` â€” Coding backend dispatch

Sends a task to whichever coding CLI is configured (Codex, Claude Code, or Gemini). The agent writes the prompt; the plugin handles backend selection, worktree setup, and output capture.

### `linear_issues` â€” Native Linear API

Agents call `linear_issues` with typed JSON parameters. The tool wraps the Linear GraphQL API directly and handles all name-to-ID resolution automatically.

| Action | What it does | Key parameters |
|---|---|---|
| `read` | Get full issue details (status, labels, comments, relations) | `issueId` |
| `create` | Create a new issue or sub-issue | `title`, `description`, `teamId` or `parentIssueId` |
| `update` | Change status, priority, labels, estimate, or title | `issueId` + fields |
| `comment` | Post a comment on an issue | `issueId`, `body` |
| `list_states` | Get available workflow states for a team | `teamId` |
| `list_labels` | Get available labels for a team | `teamId` |

**Sub-issues:** Use `action="create"` with `parentIssueId` to create sub-issues under an existing issue. The new issue inherits `teamId` and `projectId` from its parent automatically. Agents are instructed to break large work into sub-issues for granular tracking â€” any task with multiple distinct deliverables should be decomposed. Auditors can also create sub-issues for remaining work when an implementation is partial.

### `spawn_agent` / `ask_agent` â€” Multi-agent orchestration

Delegate work to other crew agents. `spawn_agent` is fire-and-forget (parallel), `ask_agent` waits for a reply (synchronous). Disabled with `enableOrchestration: false`.

### `dispatch_history` â€” Recent dispatch context

Returns recent dispatch activity. Agents use this for situational awareness when working on related issues.

### Access model

Not all agents get write access. The webhook prompts enforce this:

| Context | `linear_issues` access | `code_run` |
|---|---|---|
| Triaged issue (In Progress, etc.) | Full (read + create + update + comment) | Yes |
| Untriaged issue (Backlog, Triage) | Read only | Yes |
| Auditor | Full (read + create + update + comment) | Yes |
| Worker (inside `code_run`) | None | N/A |

---

## Linear API & Hook Architecture

This section documents every interaction between the plugin and the Linear GraphQL API, the webhook event routing, the hook lifecycle, and the dispatch pipeline internals.

### GraphQL API Layer

All Linear API calls go through `LinearAgentApi` (`src/api/linear-api.ts`), which wraps `https://api.linear.app/graphql` with automatic token refresh, retry resilience, and 401 recovery.

**Token resolution** (`resolveLinearToken`) checks three sources in priority order:

1. `pluginConfig.accessToken` â€” static config
2. Auth profile store (`~/.openclaw/auth-profiles.json`) â€” OAuth tokens with auto-refresh
3. `LINEAR_ACCESS_TOKEN` / `LINEAR_API_KEY` environment variable

OAuth tokens get a `Bearer` prefix; personal API keys do not. Tokens are refreshed 60 seconds before expiry via `refreshLinearToken()`, and the refreshed credentials are persisted back to the auth profile store.

**API methods by category:**

| Category | Method | GraphQL Operation | Used By |
|---|---|---|---|
| **Issues** | `getIssueDetails(issueId)` | `query Issue` | Triage, audit, close, `linear_issues` tool |
| | `createIssue(input)` | `mutation IssueCreate` | Planner |
| | `updateIssue(issueId, input)` | `mutation IssueUpdate` | Triage (labels, estimate, priority) |
| | `updateIssueExtended(issueId, input)` | `mutation IssueUpdate` | `linear_issues` tool, close handler |
| | `createIssueRelation(input)` | `mutation IssueRelationCreate` | Planner (dependency DAG) |
| **Comments** | `createComment(issueId, body, opts)` | `mutation CommentCreate` | All phases (fallback delivery) |
| | `createReaction(commentId, emoji)` | `mutation ReactionCreate` | Acknowledgment reactions |
| **Sessions** | `createSessionOnIssue(issueId)` | `mutation AgentSessionCreateOnIssue` | Comment handler, close handler |
| | `emitActivity(sessionId, content)` | `mutation AgentActivityCreate` | Primary response delivery |
| | `updateSession(sessionId, input)` | `mutation AgentSessionUpdate` | External URLs, plan text |
| **Teams** | `getTeamStates(teamId)` | `query TeamStates` | `linear_issues` tool, close handler |
| | `getTeamLabels(teamId)` | `query TeamLabels` | `linear_issues` tool, triage |
| | `getTeams()` | `query Teams` | Doctor health check |
| | `createLabel(teamId, name, opts)` | `mutation IssueLabelCreate` | Triage (auto-create labels) |
| **Projects** | `getProject(projectId)` | `query Project` | Planner |
| | `getProjectIssues(projectId)` | `query ProjectIssues` | Planner, DAG dispatch |
| **Webhooks** | `listWebhooks()` | `query Webhooks` | Doctor, webhook setup CLI |
| | `createWebhook(input)` | `mutation WebhookCreate` | Webhook setup CLI |
| | `updateWebhook(id, input)` | `mutation WebhookUpdate` | Webhook management |
| | `deleteWebhook(id)` | `mutation WebhookDelete` | Webhook cleanup |
| **Notifications** | `getAppNotifications(count)` | `query Notifications` | Doctor (connectivity check) |
| **Identity** | `getViewerId()` | `query Viewer` | Self-comment filtering |

### Webhook Event Routing

The plugin registers an HTTP route at `/linear/webhook` that receives POST payloads from two Linear webhook sources:

1. **Workspace webhook** â€” `Comment.create`, `Issue.update`, `Issue.create`
2. **OAuth app webhook** â€” `AgentSessionEvent.created`, `AgentSessionEvent.prompted`

Both must point to the same URL. `AgentSessionEvent` payloads carry workspace/team guidance which is extracted, cached per-team, and appended to all agent prompts. Comment webhook paths use the cached guidance since Linear does not include guidance in `Comment.create` payloads. See [Linear Guidance](#linear-guidance).

The handler dispatches by `type + action`:

```
Incoming POST /linear/webhook
  â”‚
  â”œâ”€ type=AgentSessionEvent, action=created
  â”‚    â””â”€ New agent session â†’ dedup â†’ scan message for @mentions â†’
  â”‚       route to mentioned agent (or default) â†’ run agent
  â”‚
  â”œâ”€ type=AgentSessionEvent, action=prompted
  â”‚    â””â”€ Follow-up message â†’ dedup â†’ scan message for @mentions â†’
  â”‚       route to mentioned agent (one-time detour, or default) â†’ resume agent
  â”‚
  â”œâ”€ type=Comment, action=create
  â”‚    â””â”€ Comment on issue â†’ filter self-comments (viewerId) â†’ dedup â†’
  â”‚       intent classify â†’ route to handler (see Intent Classification below)
  â”‚
  â”œâ”€ type=Issue, action=update
  â”‚    â””â”€ Issue field changed â†’ check assignment â†’ if assigned to app user â†’
  â”‚       dispatch (triage or full implementation)
  â”‚
  â”œâ”€ type=Issue, action=create
  â”‚    â””â”€ New issue created â†’ triage (estimate, labels, priority)
  â”‚
  â””â”€ type=AppUserNotification
       â””â”€ Immediately discarded (duplicates workspace webhook events)
```

### Intent Classification

When a `Comment.create` event arrives, the plugin classifies the user's intent using a two-tier system:

1. **LLM classifier** (~300 tokens, ~2-5s) â€” a small/fast model parses the comment and returns structured JSON with intent + reasoning
2. **Regex fallback** â€” if the LLM call fails or times out, static patterns catch common cases

| Intent | Trigger | Handler |
|---|---|---|
| `plan_start` | "let's plan the features" | Start planner interview session |
| `plan_finalize` | "looks good, ship it" | Run plan audit + cross-model review |
| `plan_abandon` | "cancel planning" | End planning session |
| `plan_continue` | Any message during active planning | Continue planner conversation |
| `ask_agent` | "@kaylee" or "hey kaylee" | Route to specific agent by name |
| `request_work` | "fix the search bug" | Dispatch to default agent |
| `question` | "what's the status?" | Agent answers without code changes |
| `close_issue` | "close this" / "mark as done" | Generate closure report + transition state |
| `general` | Noise, automated messages | Silently dropped |

### Hook Lifecycle

The plugin registers three lifecycle hooks via `api.on()` in `index.ts`:

**`agent_end`** â€” Dispatch pipeline state machine. When a sub-agent (worker or auditor) finishes:
- Looks up the session key in dispatch state to find the active dispatch
- Validates the attempt number matches (rejects stale events from old retries)
- If the worker finished â†’ triggers the audit phase (`triggerAudit`)
- If the auditor finished â†’ processes the verdict (`processVerdict` â†’ pass/fail/stuck)

**`before_agent_start`** â€” Context injection. For `linear-worker-*` and `linear-audit-*` sessions:
- Reads dispatch state and finds up to 3 active dispatches
- Prepends a `<dispatch-history>` block so the agent has situational awareness of concurrent work

**`message_sending`** â€” Narration guard. Catches short (~250 char) "Let me explore..." responses where the agent narrates intent without actually calling tools:
- Appends a warning: "Agent acknowledged but may not have completed the task"
- Prevents users from thinking the agent did something when it only said it would

### Response Delivery

Agent responses follow an **emitActivity-first** pattern:

1. Try `emitActivity(sessionId, { type: "response", body })` â€” appears as agent activity in Linear's UI, no duplicate comment
2. If `emitActivity` fails (no session, API error) â†’ fall back to `createComment(issueId, body)`
3. Comments posted outside sessions use `createCommentWithDedup()` â€” pre-registers the comment ID to prevent the echo webhook from triggering reprocessing

### Close Issue Flow

When intent classification returns `close_issue`:

```
close_issue intent
  â”‚
  â”œâ”€ Fetch full issue details (getIssueDetails)
  â”œâ”€ Find team's "completed" state (getTeamStates â†’ type=completed)
  â”œâ”€ Create agent session on issue (createSessionOnIssue)
  â”œâ”€ Emit "preparing closure report" thought (emitActivity)
  â”œâ”€ Run agent in read-only mode to generate closure report (runAgent)
  â”œâ”€ Transition issue state to completed (updateIssue â†’ stateId)
  â””â”€ Post closure report (emitActivity â†’ createComment fallback)
```

This is a **static action** â€” the intent triggers direct API calls orchestrated by the plugin, not by giving the agent write tools. The agent only generates the closure report text; all state transitions are handled by the plugin.

### Dispatch Pipeline Internals

The full dispatch flow for implementing an issue:

```
Issue assigned to app user
  â”‚
  â”œâ”€ 1. Assess complexity tier (runAgent â†’ junior/medior/senior)
  â”œâ”€ 2. Create isolated git worktree (createWorktree)
  â”œâ”€ 3. Register dispatch in state file (registerDispatch)
  â”œâ”€ 4. Write .claw/manifest.json with issue metadata
  â”œâ”€ 5. Notify: "dispatched as {tier}"
  â”‚
  â”œâ”€ 6. Worker phase (spawnWorker)
  â”‚    â”œâ”€ Build prompt from prompts.yaml (worker.system + worker.task)
  â”‚    â”œâ”€ If retry: append rework.addendum with prior audit gaps
  â”‚    â”œâ”€ Tool access: code_run YES, linear_issues NO
  â”‚    â””â”€ Output captured as text â†’ saved to .claw/worker-{attempt}.md
  â”‚
  â”œâ”€ 7. Audit phase (triggerAudit)
  â”‚    â”œâ”€ Build prompt from prompts.yaml (audit.system + audit.task)
  â”‚    â”œâ”€ Tool access: code_run YES, linear_issues READ+WRITE
  â”‚    â”œâ”€ Auditor verifies acceptance criteria, runs tests, reviews diff
  â”‚    â””â”€ Must return JSON verdict: {pass, criteria, gaps, testResults}
  â”‚
  â””â”€ 8. Verdict (processVerdict)
       â”œâ”€ PASS â†’ updateIssue(stateId=Done), post summary, notify âœ…
       â”œâ”€ FAIL + retries left â†’ back to step 6 with audit gaps as context
       â””â”€ FAIL + no retries â†’ escalate, notify ğŸš¨, status="stuck"
```

**State persistence:** Dispatch state is written to `~/.openclaw/linear-dispatch-state.json` with active dispatches, completed history, session mappings, and processed event IDs.

**Watchdog:** A configurable inactivity timer (`inactivitySec`, default 120s) monitors agent output. If no tool calls or text output for the configured period, the agent process is killed and retried once. If the retry also times out, the dispatch is escalated.

### `linear_issues` Tool â†’ API Mapping

The `linear_issues` registered tool translates agent requests into `LinearAgentApi` method calls:

| Tool Action | API Methods Called |
|---|---|
| `read` | `getIssueDetails(issueId)` |
| `create` | `getIssueDetails(parentIssueId)` (if parent) â†’ `getTeamLabels` (if labels) â†’ `createIssue(input)` |
| `update` | `getIssueDetails` â†’ `getTeamStates` (if status) â†’ `getTeamLabels` (if labels) â†’ `updateIssueExtended` |
| `comment` | `createComment(issueId, body)` |
| `list_states` | `getTeamStates(teamId)` |
| `list_labels` | `getTeamLabels(teamId)` |

The `update` action's key feature is **name-to-ID resolution**: agents say `status: "In Progress"` and the tool automatically resolves it to the correct `stateId` via `getTeamStates`. Same for labels â€” `labels: ["bug", "urgent"]` resolves to `labelIds` via `getTeamLabels`. Case-insensitive matching with descriptive errors when names don't match.

The `create` action supports **sub-issue creation** via `parentIssueId`. When provided, the new issue inherits `teamId` and `projectId` from the parent, and the `GraphQL-Features: sub_issues` header is sent automatically. Agents are instructed to decompose large tasks into sub-issues for granular planning and parallel dispatch.

---

## Testing & Verification

### Health check

Run the doctor to verify your setup. It checks auth, config, prompts, connectivity, and dispatch health â€” and tells you exactly how to fix anything that's wrong:

```bash
openclaw openclaw-linear doctor
```

Example output:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Linear Plugin Doctor              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Authentication & Tokens
  âœ” Access token found (source: profile)
  âœ” Token not expired (23h remaining)
  âœ” API reachable â€” logged in as Test (TestOrg)

  Agent Configuration
  âœ” agent-profiles.json loaded (2 agents)
  âœ” Default agent: coder

  Coding Tools
  âœ” coding-tools.json loaded (default: codex)
  âœ” codex: found at /usr/local/bin/codex

  Files & Directories
  âœ” Dispatch state: 1 active, 5 completed
  âœ” Prompts valid (5/5 sections, 4/4 variables)

  Connectivity
  âœ” Linear API reachable
  âœ” Webhook gateway responding

  Dispatch Health
  âœ” No stale dispatches
  âš  2 completed dispatches older than 7 days
    â†’ Run: openclaw openclaw-linear doctor --fix to clean up

  Summary: 11 passed, 1 warning, 0 errors
```

Every warning and error includes a `â†’` line telling you what to do. Run `doctor --fix` to auto-repair what it can.

### Code-run health check

For deeper diagnostics on coding tool backends (Claude Code, Codex, Gemini CLI), run the dedicated code-run doctor. It checks binary installation, API key configuration, and actually invokes each backend to verify it can authenticate and respond:

```bash
openclaw openclaw-linear code-run doctor
```

Example output:

```
Code Run: Claude Code (Anthropic)
  âœ“ Binary: 2.1.50 (/home/claw/.npm-global/bin/claude)
  âœ“ API key: configured (ANTHROPIC_API_KEY)
  âœ“ Live test: responded in 3.2s

Code Run: Codex (OpenAI)
  âœ“ Binary: 0.101.0 (/home/claw/.npm-global/bin/codex)
  âœ“ API key: configured (OPENAI_API_KEY)
  âœ“ Live test: responded in 2.8s

Code Run: Gemini CLI (Google)
  âœ“ Binary: 0.28.2 (/home/claw/.npm-global/bin/gemini)
  âœ“ API key: configured (GEMINI_API_KEY)
  âœ“ Live test: responded in 4.1s

Code Run: Routing
  âœ“ Default backend: codex
  âœ“ Mal â†’ codex (default)
  âœ“ Kaylee â†’ codex (default)
  âœ“ Inara â†’ claude (override)
  âœ“ Callable backends: 3/3
```

This is separate from the main `doctor` because each live test spawns a real CLI subprocess (~5-10s per backend). Use `--json` for machine-readable output.

### Unit tests

551 tests covering the full pipeline â€” triage, dispatch, audit, planning, intent classification, native issue tools, cross-model review, notifications, and infrastructure:

```bash
cd ~/claw-extensions/linear
npx vitest run                   # Run all tests
npx vitest run --reporter=verbose  # See every test name
npx vitest run src/pipeline/     # Just pipeline tests
```

### UAT (live integration tests)

The UAT script runs against your real Linear workspace. It creates actual issues, triggers the pipeline, and verifies the results.

```bash
# Run all UAT scenarios
npx tsx scripts/uat-linear.ts

# Run a specific scenario
npx tsx scripts/uat-linear.ts --test dispatch
npx tsx scripts/uat-linear.ts --test planning
npx tsx scripts/uat-linear.ts --test mention
npx tsx scripts/uat-linear.ts --test intent
```

**What each scenario does:**

#### `--test dispatch` (Single issue, full pipeline)

1. Creates a test issue in Linear
2. Assigns it to the agent
3. Waits for the dispatch comment (confirms the agent picked it up)
4. Waits for the audit result (pass, fail, or escalation)
5. Reports success/failure with timing

**Expected output:**

```
[dispatch] Created issue ENG-200: "UAT: simple config tweak"
[dispatch] Assigned to agent â€” waiting for dispatch comment...
[dispatch] âœ” Dispatch confirmed (12s) â€” assessed as junior
[dispatch] Waiting for audit result...
[dispatch] âœ” Audit passed (94s) â€” issue marked done
[dispatch] Total: 106s
```

#### `--test planning` (Project planning flow)

1. Creates a root issue in a test project
2. Posts `plan this project` comment
3. Waits for the planner's welcome message
4. Posts feature requirements
5. Waits for the planner to create issues
6. Posts `finalize plan`
7. Waits for plan approval or failure

**Expected output:**

```
[planning] Created project "UAT Planning Test"
[planning] Posted "plan this project" â€” waiting for welcome...
[planning] âœ” Welcome received (8s)
[planning] Posted feature description â€” waiting for response...
[planning] âœ” Planner created 3 issues (15s)
[planning] Posted "finalize plan" â€” waiting for audit...
[planning] âœ” Plan approved (6s) â€” 3 issues queued for dispatch
[planning] Total: 29s
```

#### `--test mention` (Agent routing)

1. Creates a test issue
2. Posts a comment mentioning a specific agent (e.g., `@kaylee`)
3. Waits for that agent to respond
4. Verifies the response came from the right agent

**Expected output:**

```
[mention] Created issue ENG-201
[mention] Posted "@kaylee analyze this issue"
[mention] âœ” Kaylee responded (18s)
[mention] Total: 18s
```

#### `--test intent` (Natural language routing)

1. Creates a test issue and posts a question (no `@mention`)
2. Verifies the bot responds (not silently dropped)
3. Posts a comment with an agent name but no `@` prefix
4. Verifies that agent responds
5. Tests plan review flow with cross-model audit

**Expected output:**

```
[intent] Created issue ENG-202
[intent] Posted "what can I do with this?" â€” waiting for response...
[intent] âœ” Bot responded to question (12s)
[intent] Posted "hey kaylee analyze this" â€” waiting for response...
[intent] âœ” Kaylee responded without @mention (15s)
[intent] Total: 27s
```

### Verify notifications

```bash
openclaw openclaw-linear notify test              # Send test to all targets
openclaw openclaw-linear notify test --channel discord  # Test one channel
openclaw openclaw-linear notify status             # Show what's configured
```

### Verify prompts

```bash
openclaw openclaw-linear prompts validate   # Check for template errors
openclaw openclaw-linear prompts show       # View the active prompts
```

---

## CLI Reference

```bash
# Auth & status
openclaw openclaw-linear auth                      # Run OAuth flow
openclaw openclaw-linear status                    # Check connection

# Worktrees
openclaw openclaw-linear worktrees                 # List active worktrees
openclaw openclaw-linear worktrees --prune <path>  # Remove a worktree

# Multi-repo
openclaw openclaw-linear repos check               # Validate paths, preview labels
openclaw openclaw-linear repos sync                # Create missing repo: labels in Linear

# Prompts
openclaw openclaw-linear prompts show              # View current prompts
openclaw openclaw-linear prompts path              # Show file path
openclaw openclaw-linear prompts validate          # Check for errors

# Notifications
openclaw openclaw-linear notify status             # Show targets & events
openclaw openclaw-linear notify test               # Test all targets
openclaw openclaw-linear notify test --channel discord  # Test one channel
openclaw openclaw-linear notify setup              # Interactive setup

# Webhooks
openclaw openclaw-linear webhooks status             # Show webhook config in Linear
openclaw openclaw-linear webhooks setup              # Auto-provision workspace webhook
openclaw openclaw-linear webhooks setup --dry-run    # Preview what would change
openclaw openclaw-linear webhooks setup --url <url>  # Use custom webhook URL
openclaw openclaw-linear webhooks delete <id>        # Delete a webhook by ID

# Dispatch
/dispatch list                                     # Active dispatches
/dispatch status <identifier>                      # Dispatch details
/dispatch retry <identifier>                       # Re-run stuck dispatch
/dispatch escalate <identifier> [reason]           # Force to stuck

# Health
openclaw openclaw-linear doctor                    # Run health checks
openclaw openclaw-linear doctor --fix              # Auto-fix issues
openclaw openclaw-linear doctor --json             # JSON output

# Code-run backends
openclaw openclaw-linear code-run doctor           # Deep check all backends (binary, API key, live test)
openclaw openclaw-linear code-run doctor --json    # JSON output
```

---

## Troubleshooting

Quick checks:

```bash
systemctl --user status openclaw-gateway        # Is the gateway running?
openclaw openclaw-linear status                  # Is the token valid?
journalctl --user -u openclaw-gateway -f         # Watch live logs
```

### Common Issues

| Problem | Fix |
|---|---|
| Agent goes silent | Watchdog auto-kills after `inactivitySec` and retries. Check logs for `Watchdog KILL`. |
| Dispatch stuck after watchdog | Both retries failed. Check `.claw/log.jsonl`. Re-assign issue to restart. |
| `code_run` uses wrong backend | Check `coding-tools.json` â€” explicit backend > per-agent > global default. Run `code-run doctor` to see routing. |
| `code_run` fails at runtime | Run `openclaw openclaw-linear code-run doctor` â€” checks binary, API key, and live callability for each backend. |
| Webhook events not arriving | Run `openclaw openclaw-linear webhooks setup` to auto-provision. Both webhooks must point to `/linear/webhook`. Check tunnel is running. |
| OAuth token expired | Auto-refreshes. If stuck, re-run `openclaw openclaw-linear auth` and restart. |
| Audit always fails | Run `openclaw openclaw-linear prompts validate` to check prompt syntax. |
| Multi-repo not detected | Markers must be `<!-- repos: name1, name2 -->`. Names must match `repos` config keys. |
| `/dispatch` not responding | Restart gateway. Check plugin loaded with `openclaw doctor`. |
| Comments ignored (no response) | Check logs for intent classification results. If classifier fails, regex fallback may not match. |
| Intent classifier slow | Set `classifierAgentId` to a small model agent (Haiku). Default uses your primary model. |
| Cross-model review fails | The reviewer model CLI must be installed. Check logs for "cross-model review unavailable". |
| Rich notifications are plain text | Set `"richFormat": true` in notifications config. |
| Gateway rejects config keys | Strict validator. Run `openclaw doctor --fix`. |

For detailed diagnostics, see [docs/troubleshooting.md](docs/troubleshooting.md).

---

## Further Reading

- [Architecture](docs/architecture.md) â€” Internal design, state machines, diagrams
- [Troubleshooting](docs/troubleshooting.md) â€” Diagnostic commands, curl examples, log analysis
- [Agents in Linear](https://linear.app/docs/agents-in-linear) â€” Linear's agent guidance system (workspace & team-level instructions)

---

## License

MIT
